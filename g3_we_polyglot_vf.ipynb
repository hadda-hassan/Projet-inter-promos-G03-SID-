{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"g3_we_polyglot_vf.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5718944c931a4b07891349203196c952":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2cf9bc59645f47c1b7f272b8a030bd91","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_781528f66efb4da0919a78774090cea8","IPY_MODEL_1d247325f4104423b21f83b3f6af49aa"]}},"2cf9bc59645f47c1b7f272b8a030bd91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"781528f66efb4da0919a78774090cea8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6ac1a2718f234043a2991e29a119d578","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":7544,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":7544,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_97b9c94a14884d5db12f99e2c8897285"}},"1d247325f4104423b21f83b3f6af49aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_89a465923f2d4b288be3b1c39550f1a7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 7544/7544 [18:06&lt;00:00,  6.95it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f99f07484bb341a0b015330ba63d2a63"}},"6ac1a2718f234043a2991e29a119d578":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"97b9c94a14884d5db12f99e2c8897285":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"89a465923f2d4b288be3b1c39550f1a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f99f07484bb341a0b015330ba63d2a63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"04d570b8862945ae91fb77f5b9c53cab":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5903764b486a48ee92a41828aac951a9","IPY_MODEL_7b0c6ca4b0f6403cbddd007c899c74b0"],"layout":"IPY_MODEL_833e71f729c54b9dbed76d477107d6d2"}},"833e71f729c54b9dbed76d477107d6d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5903764b486a48ee92a41828aac951a9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_1fe8cab4b1d14ee4a7839ca0a836529e","max":7534,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4bd9b8d9e35e4b34b3946759f631be1f","value":7534}},"7b0c6ca4b0f6403cbddd007c899c74b0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6867f8f39ba44c83b0c7810e8c785cd1","placeholder":"​","style":"IPY_MODEL_cf9ea91f0e3c487f805f8a59cd9152e7","value":" 7534/7534 [01:41&lt;00:00, 74.58it/s]"}},"4bd9b8d9e35e4b34b3946759f631be1f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"1fe8cab4b1d14ee4a7839ca0a836529e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf9ea91f0e3c487f805f8a59cd9152e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6867f8f39ba44c83b0c7810e8c785cd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"24H6aJ1RlHsS"},"source":["Created on Thursday 14 January 2021  \n","\n","**Group 3 - Representation**  \n","**The objective of this notebook is to compute weighted (from IDF Inverse Document Frequency) word embedding for each art_content with the polyglot model** \n","\n","@authors : Arthur CARLET, Guillaume BERNARD, Neima MARCO, Nesrine AIDER, Lou-Ann CHAUSSE, Fannie MATHEY"]},{"cell_type":"markdown","metadata":{"id":"TEEYC317W8MP"},"source":["# Libraries"]},{"cell_type":"markdown","metadata":{"id":"QFPfOHgAXOn3"},"source":["## Install :"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZMpMPG5uW0Ue","outputId":"07339d2b-824e-40f5-dd04-c28edfbbdd2d"},"source":["# Polyglot's model installation :\n","\n","!pip install icu\n","!pip install pyicu\n","!pip install pycld2\n","!pip install morfessor\n","!pip install -U polyglot\n","!polyglot download embeddings2.fr\n","!polyglot download pos2.fr\n","!polyglot download sgns2.fr\n","\n","#lemmatizer\n","!pip install git+https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting icu\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/d8/0972fa39747faea092e8105103f261e01d6cefe262cbe036df8b0b8ada44/icu-0.0.1-py3-none-any.whl (49kB)\n","\r\u001b[K     |██████▋                         | 10kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 20kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 30kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 40kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 3.1MB/s \n","\u001b[?25hInstalling collected packages: icu\n","Successfully installed icu-0.0.1\n","Collecting pyicu\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/46/fa08c8efae2951e67681ec24319f789fc1a74e2096dd74373e34c79319de/PyICU-2.6.tar.gz (233kB)\n","\u001b[K     |████████████████████████████████| 235kB 6.6MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyicu\n","  Building wheel for pyicu (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyicu: filename=PyICU-2.6-cp36-cp36m-linux_x86_64.whl size=1288278 sha256=15aac880bbe36d42e772ead85f6bacbf6e3012da8a6b3a4d86d188991d96ff4e\n","  Stored in directory: /root/.cache/pip/wheels/31/21/2f/1c91831e8a93537ab21f6b4b935781b681104635fdb0315791\n","Successfully built pyicu\n","Installing collected packages: pyicu\n","Successfully installed pyicu-2.6\n","Collecting pycld2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/d2/8b0def84a53c88d0eb27c67b05269fbd16ad68df8c78849e7b5d65e6aec3/pycld2-0.41.tar.gz (41.4MB)\n","\u001b[K     |████████████████████████████████| 41.4MB 106kB/s \n","\u001b[?25hBuilding wheels for collected packages: pycld2\n","  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycld2: filename=pycld2-0.41-cp36-cp36m-linux_x86_64.whl size=9833529 sha256=f2f4c482f0f15d07b8972837de446d040ec5ca03c3d3aeb493fea15a34a173df\n","  Stored in directory: /root/.cache/pip/wheels/c6/8f/e9/08a1a8932a490175bd140206cd86a3dbcfc70498100de11079\n","Successfully built pycld2\n","Installing collected packages: pycld2\n","Successfully installed pycld2-0.41\n","Collecting morfessor\n","  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n","Installing collected packages: morfessor\n","Successfully installed morfessor-2.0.6\n","Collecting polyglot\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/98/e24e2489114c5112b083714277204d92d372f5bbe00d5507acf40370edb9/polyglot-16.7.4.tar.gz (126kB)\n","\u001b[K     |████████████████████████████████| 133kB 6.7MB/s \n","\u001b[?25hBuilding wheels for collected packages: polyglot\n","  Building wheel for polyglot (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for polyglot: filename=polyglot-16.7.4-py2.py3-none-any.whl size=52557 sha256=eebf5d39ea9bfdb90b1ba67b1de367a146276cc6296e529b446d61230a165dbd\n","  Stored in directory: /root/.cache/pip/wheels/5e/91/ef/f1369fdc1203b0a9347d4b24f149b83a305f39ab047986d9da\n","Successfully built polyglot\n","Installing collected packages: polyglot\n","Successfully installed polyglot-16.7.4\n","[polyglot_data] Downloading package embeddings2.fr to\n","[polyglot_data]     /root/polyglot_data...\n","[polyglot_data] Downloading package pos2.fr to /root/polyglot_data...\n","[polyglot_data] Downloading package sgns2.fr to /root/polyglot_data...\n","Collecting git+https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git\n","  Cloning https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git to /tmp/pip-req-build-_dl1u8_5\n","  Running command git clone -q https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git /tmp/pip-req-build-_dl1u8_5\n","Building wheels for collected packages: FrenchLefffLemmatizer\n","  Building wheel for FrenchLefffLemmatizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for FrenchLefffLemmatizer: filename=FrenchLefffLemmatizer-0.3-cp36-none-any.whl size=3533520 sha256=b23bbb65b3546f081f87ba44d9ed308cdda883f8da8f174d0da3336e9914dc9f\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-c2iqwfs5/wheels/95/b7/c0/e249ca2690c04f6106b9581c5e4111287f71dbd85bac903445\n","Successfully built FrenchLefffLemmatizer\n","Installing collected packages: FrenchLefffLemmatizer\n","Successfully installed FrenchLefffLemmatizer-0.3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lZH-mO_cXRZe"},"source":["## Imports :"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wGlLQ9U2lHsa","outputId":"6367d58f-38c0-4e64-f8f8-24f9944f9eee"},"source":["#imports\n","\n","import pandas as pd\n","import numpy as np\n","from tqdm.notebook import tqdm\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from string import punctuation \n","import pickle\n","\n","# Polyglot : \n","from icu import Locale\n","import polyglot\n","from polyglot.text import Text, Word\n","\n","#nltk\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('words')\n","from nltk.corpus import words,wordnet,stopwords\n","from nltk.tokenize import word_tokenize\n","\n","# Gensim\n","import gensim\n","from gensim.models import Word2Vec\n","\n","#lemmatizer\n","from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FX-O8ZoYXbjI"},"source":["# Data Import"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qPTnZlAjChvN","outputId":"61fa8472-8119-491f-f6c8-6f0f7ad533f3"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HV1eovyAXdI0"},"source":["# Data loading\n","DATA_PATH = '/content/drive/MyDrive/PIP 2021/Données//Deduplicated'\n","df = pd.read_json(DATA_PATH + '/df_concat_G1_G2_v0.json')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BbMLV3QRXho8"},"source":["# Lemmatization"]},{"cell_type":"code","metadata":{"id":"_aW_RNJrXgyl"},"source":["# Loading stopwords\n","stop = stopwords.words('french')\n","\n","lemmatizer = FrenchLefffLemmatizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ajYYNtbVXndJ"},"source":["def preprocess(sentence: str) -> list:\n","    \"\"\"Documentation\n","    Parameters:\n","        sentence: sentence to preprocess \n","\n","    Out:\n","        sentence: sentence preprocessed\n","    \"\"\"\n","    sentence = str(sentence.lower())\n","    sentence = nltk.word_tokenize(sentence)\n","    sentence = [word_sen for word_sen in sentence if word_sen not in stop and word_sen.isalpha() and len(word_sen) > 2]\n","    return sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hTqbox3DPLce"},"source":["dictio = {'VERB': 'v', 'ADJ': 'adj', 'DET': 'det', 'NOUN': 'nc', 'AUX': 'v', 'ADP': 'prep', 'ADV': 'adv', 'CONJ': 'coo',\n","          'INTJ': 'nc', 'NUM': 'nc', 'PART': 'nc', 'PRON': 'cln', 'PUNCT': 'poncts', 'PROPN': 'np', 'SCONJ': 'csu', 'SYM': 'nc', 'X': 'nc'}\n","\n","\n","def lem_word(word: str) -> (str, bool, np.ndarray):\n","    \"\"\"Documentation\n","    Parameters:\n","        word: word to lemmatize \n","\n","    Out:\n","        resu: word lemmatized\n","        is_valid: wether or not the word is part of polyglot's vocab\n","        vect: word's embedding if isValid is True, else it returns a vector full of zeroes\n","\n","    References:\n","        1. https://spacy.io/universe/project/spacy-lefff\n","        2. https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer\n","        3. https://polyglot.readthedocs.io/en/latest/\n","    \"\"\"\n","    # default values initialisation\n","    resu = word\n","    vect = np.zeros(256)\n","    is_valid = False\n","\n","    # computes pos_tag and word embedding\n","    text = Text(word, hint_language_code='fr')\n","\n","    try:  # crashes if word isn't in model's vocab\n","        pos_tag = text.pos_tags[0][1]\n","        tag = dictio[pos_tag]\n","        resu = lemmatizer.lemmatize(text, tag) #we need pos_tag to make this function work\n","        if resu != '':  # different cases of pos tagging output\n","            if (type(resu) == list):\n","                if len(resu) == 0:\n","                    resu = ''\n","                else:\n","                    is_valid = True\n","                    resu = resu[0][0]\n","            else:\n","                isValid = True\n","        if is_valid:\n","            vect = np.array(Word(resu, language='fr').vector)\n","        return resu, is_valid, vect\n","    except:\n","        return resu, is_valid, vect"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x8OXgpMlEW-m"},"source":["# Word Embedding "]},{"cell_type":"code","metadata":{"id":"0kp2B503Vxdf"},"source":["word_to_lem = {}  # dict that takes a word as input and returns its lemmatized representation\n","\n","\n","def compute_word_embeddings(df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n","    \"\"\"Documentation\n","    Parameters:\n","        df: dataframe containing the content\n","\n","    Out:\n","        df_res : dataframe containing id,content,list of words in the content, all embedding linked to\n","                the vector and the content lemmatized\n","    \"\"\"\n","    # initialisation\n","    art_word = []\n","    art_lem = []\n","    art_vect = []\n","    art_lem_join = [] #we need to stocks the sentence lemmatized to compute idf\n","\n","    for content in tqdm(df['art_content']):\n","        words = preprocess(content)\n","        words_new = []  # contains words known from polyglot\n","        words_vect = []  # contains lemmatized words\n","        words_lem = []  # contains embedding of lemmatized words\n","        for w in words:\n","            lem, is_valid, w_embedding = lem_word(w)  # resu, isValid, vect\n","            if is_valid:  # if word in polglot's vocabulary\n","                words_new.append(w)\n","                if w not in word_to_lem.keys():  # dictionnary that take a word as input and returns its lemma\n","                    word_to_lem[w] = lem\n","                words_lem.append(str(lem))\n","                words_vect.append(w_embedding)\n","        art_word.append(words_new)\n","        words_join = ' '.join(words_lem)\n","        art_lem_join.append(words_join)\n","        art_lem.append(words_lem)\n","        art_vect.append(words_vect)\n","    df_res = pd.DataFrame({'art_id': df['art_id'], 'art_content': df['art_content'],\n","                           'art_word': art_word, 'art_lem': art_lem, 'art_vect': art_vect, 'art_lem_join': art_lem_join})\n","    return df_res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["5718944c931a4b07891349203196c952","2cf9bc59645f47c1b7f272b8a030bd91","781528f66efb4da0919a78774090cea8","1d247325f4104423b21f83b3f6af49aa","6ac1a2718f234043a2991e29a119d578","97b9c94a14884d5db12f99e2c8897285","89a465923f2d4b288be3b1c39550f1a7","f99f07484bb341a0b015330ba63d2a63"]},"id":"Dpn68bx1TOBp","outputId":"45c2d9e4-fc9a-4d95-c68f-53d42c406100"},"source":["df_with_embeddings = compute_word_embeddings(df)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5718944c931a4b07891349203196c952","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=7544.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":264},"id":"cNhp1z3OTsjb","scrolled":true,"outputId":"fcc48156-de95-4278-8fb9-050a03e005a2"},"source":["df_with_embeddings.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>art_id</th>\n","      <th>art_content</th>\n","      <th>art_word</th>\n","      <th>art_lem</th>\n","      <th>art_vect</th>\n","      <th>art_lem_join</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>La FNCDG et l’ANDCDG ont publié en septembre l...</td>\n","      <td>[édition, panorama, emploi, territorial, cette...</td>\n","      <td>[édition, panorama, emploi, territorial, ce, é...</td>\n","      <td>[[3.3952472, -4.7512593, -0.87234586, 3.186644...</td>\n","      <td>édition panorama emploi territorial ce édition...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Malgré la levée des mesures de confinement le ...</td>\n","      <td>[malgré, levée, mesures, confinement, mai, plu...</td>\n","      <td>[malgré, levée, mesure, confinement, mai, plup...</td>\n","      <td>[[-0.2212756, 0.57220876, -0.5409405, 0.656040...</td>\n","      <td>malgré levée mesure confinement mai plupart me...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>25</td>\n","      <td>Quels étaient les objectifs poursuivis par le ...</td>\n","      <td>[objectifs, gouvernement, cadre, cette, réform...</td>\n","      <td>[objectif, gouvernement, cadre, ce, réforme, f...</td>\n","      <td>[[2.6337967, 1.5813266, 1.1626679, -0.25885695...</td>\n","      <td>objectif gouvernement cadre ce réforme fonctio...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>27</td>\n","      <td>La journée thématique, qui aura lieu durant le...</td>\n","      <td>[journée, thématique, lieu, durant, salon, thè...</td>\n","      <td>[journée, thématique, lieu, durant, salon, thè...</td>\n","      <td>[[2.9263377, -0.33556777, -1.4783581, 2.622460...</td>\n","      <td>journée thématique lieu durant salon thème ser...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>28</td>\n","      <td>La 1ère journée thématique en région sur le th...</td>\n","      <td>[journée, thématique, région, thème, vers, nou...</td>\n","      <td>[journée, thématique, région, thème, vers, nou...</td>\n","      <td>[[2.9263377, -0.33556777, -1.4783581, 2.622460...</td>\n","      <td>journée thématique région thème vers nouveau m...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  art_id  ...                                       art_lem_join\n","0      1  ...  édition panorama emploi territorial ce édition...\n","1      2  ...  malgré levée mesure confinement mai plupart me...\n","2     25  ...  objectif gouvernement cadre ce réforme fonctio...\n","3     27  ...  journée thématique lieu durant salon thème ser...\n","4     28  ...  journée thématique région thème vers nouveau m...\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"kcPIIPuAlHsd"},"source":["## Compute IDF for each stemmed word"]},{"cell_type":"markdown","metadata":{"id":"5M6EvNFulHsd"},"source":["We want to penalize the embedding for words that are too common. To do that we will make weigth based on their IDF values"]},{"cell_type":"code","metadata":{"id":"grzM9345X1aX"},"source":["# Calculation of the idf value of each word\n","vectorizer = TfidfVectorizer()\n","\n","# we use the lemmatized sentence to compute tf-idf in order to have a same idf for same word\n","tfidf_matrix = vectorizer.fit_transform(df_with_embeddings['art_lem_join'])\n","\n","# Dictionary containing the idf value of each word\n","dic_weights = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"esBAgieCZo0M","outputId":"bc1c7110-6b2b-4503-8aea-996c140d9d52"},"source":["len(dic_weights.keys()) #approximately 10 000 unique lemmatized words"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10755"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"Bnvl52cOZ1nn"},"source":["def get_idf(word: str) -> float:\n","    \"\"\"Documentation\n","    Parameters:\n","        word: the word we want to get the idf \n","\n","    Out:\n","        float: the idf value\n","    \"\"\"\n","    if word in word_to_lem.keys():\n","        # dictionnary that take a word as input and returns its lemma, was filled in compute_word_embeddings function\n","        word_lemmatized = word_to_lem[word]\n","        return dic_weights[word_lemmatized]\n","    else:\n","        return 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-y4GrjkMZ3Ck"},"source":["def get_weight_vector(tokens: list) -> np.ndarray:\n","    \"\"\"Documentation\n","    Parameters:\n","        tokens: list of words in a list\n","\n","    Out:\n","        float: array of idf values\n","    \"\"\"\n","    return np.array([get_idf(token) for token in tokens])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oVVGh8dalHsf"},"source":["#save tfidf model\n","with open('/content/drive/MyDrive/PIP 2021/Pos Tagging/Guillaume/tfidf_lem.pickle', 'wb') as f1:\n","    pickle.dump(vectorizer, f1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wC5nbxCalHsf"},"source":["## Weigthing the word embeddings"]},{"cell_type":"code","metadata":{"id":"whwkgly2iJww"},"source":["def compute_weighted_average_embeddings(row: pd.Series):\n","    \"\"\"Documentation\n","    Parameters:\n","        row: row from the dataframe containing the lemmatized sentence\n","            and the embedding for each words of the sentence\n","\n","    Out:\n","        list: array of embedding values\n","\n","    References:\n","        https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n","\n","    \"\"\"\n","    lem_words = row[\"art_lem\"]\n","    embedded_sentence = row['art_vect']\n","    weights = [get_idf(word) for word in lem_words]\n","    if np.array_equal(weights, np.zeros(len(weights))):\n","        sentence_embedding = np.zeros(len(weights))\n","    else:\n","        sentence_embedding = np.average(\n","            embedded_sentence, weights=weights, axis=0)\n","    return sentence_embedding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["04d570b8862945ae91fb77f5b9c53cab","833e71f729c54b9dbed76d477107d6d2","5903764b486a48ee92a41828aac951a9","7b0c6ca4b0f6403cbddd007c899c74b0","4bd9b8d9e35e4b34b3946759f631be1f","1fe8cab4b1d14ee4a7839ca0a836529e","cf9ea91f0e3c487f805f8a59cd9152e7","6867f8f39ba44c83b0c7810e8c785cd1"]},"id":"KnHoGyem8sd3","outputId":"84229f6c-1fc4-4154-e64e-dec54d23ce22"},"source":["df_with_embeddings['word_embedding'] = [compute_weighted_average_embeddings(\n","    df_with_embeddings.iloc[x]) for x in tqdm(range(len(df_with_embeddings)))]"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"04d570b8862945ae91fb77f5b9c53cab","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=7534.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5egIyIaLYl2F"},"source":["output=df_final_2[[\"art_id\", \"word_embedding\"]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":197},"id":"WfgajVGQY2Af","outputId":"730ee671-f60a-44ca-8a43-6f60d0d4ef75"},"source":["output.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>art_id</th>\n","      <th>word_embedding</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>[2.224703109314392, 0.6259871201526235, -0.212...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>[2.0636399535205308, 0.9396294129786164, 0.120...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>25</td>\n","      <td>[1.6940103703618323, 1.2793173774418638, 0.317...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>27</td>\n","      <td>[2.127060536324472, 0.7990025027405475, -0.163...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>28</td>\n","      <td>[1.5101993388851078, 0.6776250067902838, 0.019...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  art_id                                     word_embedding\n","0      1  [2.224703109314392, 0.6259871201526235, -0.212...\n","1      2  [2.0636399535205308, 0.9396294129786164, 0.120...\n","2     25  [1.6940103703618323, 1.2793173774418638, 0.317...\n","3     27  [2.127060536324472, 0.7990025027405475, -0.163...\n","4     28  [1.5101993388851078, 0.6776250067902838, 0.019..."]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"QE4lu1obYX44"},"source":["df_final_2[[\"art_id\", \"word_embedding\"]].to_json(\n","    '/content/drive/MyDrive/PIP 2021/Données/polyglot_embeddings_lem.json', orient=\"records\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uqcBXHOLlHsg"},"source":["---"]}]}