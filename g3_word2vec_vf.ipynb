{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"g3_word2vec_vf.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"P6onfXUt6Txt"},"source":["Created on January 7 January 2021  \n","\n","**Group 3 - Representation**  \n","**The objective of this notebook is to create a representation of our data using a gensim model** \n","\n","@author : Jules Boutibou"]},{"cell_type":"markdown","metadata":{"id":"k8KXdkvEuwS6"},"source":["# Libraries"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B-uU3-AUCAZy","executionInfo":{"status":"ok","timestamp":1610964274349,"user_tz":-60,"elapsed":4698,"user":{"displayName":"Jules Boutibou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggyc5dNDCMFVo6HX5idpNboNRlAei1MSIztxX79Yg=s64","userId":"04206329469966522988"}},"outputId":"dd4c641b-0afa-462a-b181-bdd4e7b01a65"},"source":["from string import punctuation\n","from tqdm import tqdm\n","from operator import itemgetter\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from gensim.models import Word2Vec\n","from nltk.corpus import stopwords\n","import pandas as pd\n","import numpy as np\n","import unicodedata\n","import gensim\n","import re\n","from google.colab import drive\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","tqdm.pandas()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-rYyentdu1Lw"},"source":["# Link to the drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cuVeU3qWtfyG","executionInfo":{"status":"ok","timestamp":1610964315933,"user_tz":-60,"elapsed":46219,"user":{"displayName":"Jules Boutibou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggyc5dNDCMFVo6HX5idpNboNRlAei1MSIztxX79Yg=s64","userId":"04206329469966522988"}},"outputId":"e3fb94e2-aee2-4d7d-d45d-3a3a8009060b"},"source":["drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nMby4Fznu7Rl"},"source":["# Word2Vec"]},{"cell_type":"code","metadata":{"id":"Owttc08ICSkH"},"source":["# Import a pre-trained model (trained on french 2018 Wikipedia).\n","# References : Found on https://zenodo.org/record/3241447#.X_Wu9elKh24\n","found_model = gensim.models.KeyedVectors.load_word2vec_format(\n","    '/content/drive/MyDrive/PIP 2021/Word Embedding/Modele/Pretrained_model/modele_simple.bin', binary=True)\n","\n","# Import the pre-trained model on all the articles and titles\n","our_model = gensim.models.KeyedVectors.load_word2vec_format(\n","    '/content/drive/MyDrive/PIP 2021/Word Embedding/Modele/Pretrained_model/model_trained_on_articles.txt', binary=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IEmWpKE4vCmm"},"source":["# Vocabulary cleaning"]},{"cell_type":"code","metadata":{"id":"W-p0G-AcsUYJ"},"source":["def strip_accents(s: str) -> str:\n","    \"\"\"\n","    Returns the sentence without accent\n","    \"\"\"\n","\n","    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1KAm5kFDGqcm"},"source":["# Remove the pos tag to only keep the french word, ant lower it\n","found_model.vocab = {strip_accents(\n","    k.split('_')[0].lower()): v for k, v in found_model.vocab.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jMznpTqfvSjn"},"source":["# French stopwords importation\n","stop = stopwords.words('french')\n","vocab = list(found_model.vocab.keys())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ianHoCTqupZn"},"source":["# Remove the vocabulary that is a french stopword\n","vocab = [v for v in vocab if v not in stop]\n","\n","# Keep only alphabet or ' character\n","vocab = [v for v in vocab if v.isalpha() or \"'\" in v]\n","\n","# Keep word after ' char\n","# i.e. l'accord --> keeps accord instead of laccord after removing punctuation\n","vocab = [v.split(\"'\")[1] if \"'\" in v else v for v in vocab]\n","\n","# Keep word that has a length bigger than 2 characters\n","vocab = [v for v in vocab if len(v) > 2]\n","\n","# Keep words existing in the original vocabulary model\n","vocab = set(vocab).intersection(set(found_model.vocab))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FViHmEbcwPQP"},"source":["# New cleaned vocabulary\n","found_model.vocab = dict(zip(vocab, itemgetter(*vocab)(found_model.vocab)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wRY7v8KcZKsN"},"source":["All of these preprocesses have already been done on the model we trained on the corpus."]},{"cell_type":"markdown","metadata":{"id":"3nPqJxVoyI0e"},"source":["# Word2Vec application on our data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"grwhmVWhyP8i","executionInfo":{"status":"ok","timestamp":1610964412254,"user_tz":-60,"elapsed":142514,"user":{"displayName":"Jules Boutibou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggyc5dNDCMFVo6HX5idpNboNRlAei1MSIztxX79Yg=s64","userId":"04206329469966522988"}},"outputId":"a8f3c196-4486-4e26-9e4c-d6c40b06bd59"},"source":["# Import the cleaned data, without lemmatization\n","data = pd.read_json('/content/drive/MyDrive/PIP 2021/Données/Deduplicated/df_concat_G1_G2_v0.json')\n","data = data[['art_id', 'art_title', 'art_content']]\n","data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>art_id</th>\n","      <th>art_title</th>\n","      <th>art_content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>9ème édition du Panorama de l’emploi territorial</td>\n","      <td>La FNCDG et l’ANDCDG ont publié en septembre l...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>ACTUALITÉS FNCDG / COVID19</td>\n","      <td>Malgré la levée des mesures de confinement le ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>25</td>\n","      <td>Interview de M. Olivier DUSSOPT, Secretaire d’...</td>\n","      <td>Quels étaient les objectifs poursuivis par le ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>27</td>\n","      <td>Journée Thématique FNCDG « Les services de san...</td>\n","      <td>La journée thématique, qui aura lieu durant le...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>28</td>\n","      <td>Journée Thématique FNCDG « Vers de nouveaux mo...</td>\n","      <td>La 1ère journée thématique en région sur le th...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  art_id  ...                                        art_content\n","0      1  ...  La FNCDG et l’ANDCDG ont publié en septembre l...\n","1      2  ...  Malgré la levée des mesures de confinement le ...\n","2     25  ...  Quels étaient les objectifs poursuivis par le ...\n","3     27  ...  La journée thématique, qui aura lieu durant le...\n","4     28  ...  La 1ère journée thématique en région sur le th...\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"cgzLoKEudXLD"},"source":["def preprocessing(sentences) -> list:\n","    \"\"\"\n","    Takes a column containing sentences, and return the processed text.\n","    Removes punctuation, stopwords, numbers, accents, white spaces, and lemmatize a text\n","    Parameters :\n","      sentences : pd.dataframe column\n","    \"\"\"\n","\n","    processed_sentences = []\n","\n","    for sentence in tqdm(sentences):\n","\n","        # Convert to lowercase\n","        sentence = sentence.lower()\n","\n","        # Remove space(begin, end)\n","        sentence = str(sentence).strip()\n","\n","        # Remove white space\n","        sentence = str(sentence).strip()\n","\n","        # Remove accent\n","        sentence = ''.join((c for c in unicodedata.normalize(\n","            'NFD', sentence) if unicodedata.category(c) != 'Mn'))\n","\n","        # Remove number\n","        sentence = ''.join([i for i in sentence if not i.isdigit()])\n","\n","        # Remove other non-alphabets symbols with space (i.e. keep only alphabets and whitespaces and char ')\n","        sentence = re.sub(\"[^a-zA-Z ']\", '', sentence)\n","\n","        words = sentence.split()\n","\n","        # Keep word after ' char\n","        # i.e. l'accord --> keeps accord instead of laccord after removing punctuation\n","        sentence = [w.split(\"'\")[1] if \"'\" in w else w for w in words]\n","\n","        # Keep words that have length of more than 2, remove those with length 1 or 2\n","        processed_sentences.append(\n","            ' '.join([w for w in sentence if len(w) > 2 and len(w) < 50]))\n","\n","    return processed_sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"io8ptfrSdaIn","executionInfo":{"status":"ok","timestamp":1610964438848,"user_tz":-60,"elapsed":169093,"user":{"displayName":"Jules Boutibou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggyc5dNDCMFVo6HX5idpNboNRlAei1MSIztxX79Yg=s64","userId":"04206329469966522988"}},"outputId":"76b02347-f60a-4c57-bf5f-91cb49e07ef1"},"source":["# Applying the cleaning function on the column art_content\n","data['art_content'] = preprocessing(data['art_content'])\n","data['art_content'] = data['art_content'].apply(\n","    lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n","\n","# Applying the cleaning function on the column art_title\n","data['art_title'] = preprocessing(data['art_title'])\n","data['art_title'] = data['art_title'].apply(lambda x: ' '.join(\n","    [word for word in x.split() if word not in (stop)]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 7544/7544 [00:19<00:00, 389.48it/s]\n","100%|██████████| 7544/7544 [00:00<00:00, 20647.13it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"CFXIgzNBEiuz"},"source":["def existing_tokens(sentence: str, model) -> list:\n","    \"\"\"\n","    Tokenize a sentence, and returns only the tokens existing in the vocabulary of the model\n","    Parameters :\n","      sentence : sentence we want to tokenize and filter \n","      model : pre-trained word2vec skip-gram model\n","    Out :\n","      list of words\n","    \"\"\"\n","\n","    # Keeps words or the article only if they are in the model vocabulary\n","    sentence = set(nltk.word_tokenize(str(sentence)))\n","    intersection = sentence.intersection(set(model.vocab.keys()))\n","    return list(intersection)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lUFwLtH5MISW"},"source":["# Apply the function existing_tokens for each duo found_model&our_model / art_content&art_title\n","data['content_found_model'] = data['art_content'].apply(\n","    lambda x: existing_tokens(x, found_model))\n","data['content_our_model'] = data['art_content'].apply(\n","    lambda x: existing_tokens(x, our_model))\n","data['title_found_model'] = data['art_title'].apply(\n","    lambda x: existing_tokens(x, found_model))\n","data['title_our_model'] = data['art_title'].apply(\n","    lambda x: existing_tokens(x, our_model))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dV2KEHURbomf"},"source":["# Remove sentences that doesn't contain any word of the corresponding vocabulary model\n","content_found_model = data[data['content_found_model'].apply(\n","    lambda x: len(x) != 0)]\n","content_our_model = data[data['content_our_model'].apply(\n","    lambda x: len(x) != 0)]\n","title_found_model = data[data['title_found_model'].apply(\n","    lambda x: len(x) != 0)]\n","title_our_model = data[data['title_our_model'].apply(lambda x: len(x) != 0)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SjbU7koJgFS6"},"source":["# Calculation of the idf value of each word\n","vectorizer_content_found_model = TfidfVectorizer()\n","vectorizer_content_our_model = TfidfVectorizer()\n","vectorizer_title_found_model = TfidfVectorizer()\n","vectorizer_title_our_model = TfidfVectorizer()\n","\n","# fit_transform needs a non-tokenized sentence\n","x_content_found_model = vectorizer_content_found_model.fit_transform(\n","    [' '.join(sentence)\n","     for sentence in content_found_model['content_found_model']]\n",")\n","x_content_our_model = vectorizer_content_our_model.fit_transform(\n","    [' '.join(sentence)\n","     for sentence in content_our_model['content_our_model']]\n",")\n","x_title_found_model = vectorizer_title_found_model.fit_transform(\n","    [' '.join(sentence)\n","     for sentence in title_found_model['title_found_model']]\n",")\n","x_title_our_model = vectorizer_title_our_model.fit_transform(\n","    [' '.join(sentence)\n","     for sentence in title_our_model['title_our_model']]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ut9EDTj-zZT6"},"source":["def dic_weights(vectorizer):\n","\n","  \"\"\" \n","  Returns a dictionnary containing corpus' words as key, and it's idf_value as value\n","  Parameters :\n","    vectorizer : sklearn.feature_extraction.text.TfidVectorizer\n","  Out :\n","    dictionary : words as key, idf value as value\n","  \"\"\"\n","\n","  return dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0uaFKW1hgGxY"},"source":["# Using the dic_weights function for each duo found_model&our_model / art_content&art_title\n","dic_content_found_model = dic_weights(vectorizer_content_found_model)\n","dic_content_our_model = dic_weights(vectorizer_content_our_model)\n","dic_title_found_model = dic_weights(vectorizer_title_found_model)\n","dic_title_our_model = dic_weights(vectorizer_title_our_model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S1J0NIfa-ZV3"},"source":["def vect_mean(sentence: list, model, dic) -> list:\n","    \"\"\"\n","    Returns a list representing the average of the array's words' sentence \n","    weighted with it's idf value\n","    Parameters :\n","      sentence : tokenized words of a sentence type list\n","      model : pre-trained word2vec skip-gram model\n","      dic : dictionary containing idf_value of each word\n","    Out :\n","      sentence embedding\n","    \"\"\"\n","\n","    # Weight of each word\n","    if len(sentence) == 1:\n","        return model[sentence][0]\n","    else:\n","        poids = list(itemgetter(*sentence)(dic))\n","        return np.average(model[sentence], axis=0, weights=poids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RcsH8wJP7wtk"},"source":["# Apply the function to calculate the embeddings for each duo found_model&our_model / art_content&art_title\n","data['vect_art_found_model'] = content_found_model.content_found_model.apply(\n","    lambda x: vect_mean(x, found_model, dic_content_found_model))\n","data['vect_art_our_model'] = content_our_model.content_our_model.apply(\n","    lambda x: vect_mean(x, our_model, dic_content_our_model))\n","data['vect_title_found_model'] = title_found_model.title_found_model.apply(\n","    lambda x: vect_mean(x, found_model, dic_title_found_model))\n","data['vect_title_our_model'] = title_our_model.title_our_model.apply(\n","    lambda x: vect_mean(x, our_model, dic_title_our_model))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nmkNr0Vq6Tx_"},"source":["# Exportation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":289},"id":"7W1l9_Yi0Eep","executionInfo":{"status":"ok","timestamp":1610968393603,"user_tz":-60,"elapsed":941,"user":{"displayName":"Jules Boutibou","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggyc5dNDCMFVo6HX5idpNboNRlAei1MSIztxX79Yg=s64","userId":"04206329469966522988"}},"outputId":"3af5a3e8-0496-401a-9b06-534c805d8087"},"source":["# Final DataFrame containing the column Id of the article (art_id) and the 4 calculated embedding\n","final_data = data[['art_id', 'vect_art_found_model', 'vect_art_our_model', 'vect_title_found_model', 'vect_title_our_model']]\n","final_data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>art_id</th>\n","      <th>vect_art_found_model</th>\n","      <th>vect_art_our_model</th>\n","      <th>vect_title_found_model</th>\n","      <th>vect_title_our_model</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>[-0.06533379122757822, 0.07057429621559858, 0....</td>\n","      <td>[-0.12480441236156328, 0.0765136591815489, 0.1...</td>\n","      <td>[-0.07113228683669678, 0.11271336567454315, 0....</td>\n","      <td>[-0.12933454550715504, 0.06898164503889337, 0....</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>[-0.07426156787665815, 0.06082738646515634, 0....</td>\n","      <td>[-0.09378982401034014, 0.07844326181610713, 0....</td>\n","      <td>[-0.07399426, 0.036537983, 0.025026318, -0.057...</td>\n","      <td>[-0.047668009415286657, -0.12919949592780508, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>25</td>\n","      <td>[-0.07710211662206251, 0.05970139053177871, 0....</td>\n","      <td>[-0.0575083978669747, 0.021040486342290866, 0....</td>\n","      <td>[-0.046578286884785186, 0.022779421749476936, ...</td>\n","      <td>[-0.10663828383062753, 0.0007694621669294691, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>27</td>\n","      <td>[-0.045228940506052764, 0.07690330944376708, 0...</td>\n","      <td>[-0.09709182256147646, -0.0007106088381618878,...</td>\n","      <td>[-0.060727528655831205, 0.10974791665845304, 0...</td>\n","      <td>[-0.05749720781059726, -0.04028034379641815, 0...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>28</td>\n","      <td>[-0.060625831064918353, 0.0752820756392532, 0....</td>\n","      <td>[-0.09045600879906518, -0.008876421196522053, ...</td>\n","      <td>[-0.0852355797657426, 0.10057970966708417, 0.0...</td>\n","      <td>[-0.031851939993983974, -0.0339284274483017, 0...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  art_id  ...                               vect_title_our_model\n","0      1  ...  [-0.12933454550715504, 0.06898164503889337, 0....\n","1      2  ...  [-0.047668009415286657, -0.12919949592780508, ...\n","2     25  ...  [-0.10663828383062753, 0.0007694621669294691, ...\n","3     27  ...  [-0.05749720781059726, -0.04028034379641815, 0...\n","4     28  ...  [-0.031851939993983974, -0.0339284274483017, 0...\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"dIS9YW2HcJ80"},"source":["# Exporting the final data\n","final_data.to_json(r'/content/drive/MyDrive/PIP 2021/Données/Word2Vec/article_and_titles_embeddings.json', orient='records')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jlunTfRA6TyA"},"source":["---"]}]}